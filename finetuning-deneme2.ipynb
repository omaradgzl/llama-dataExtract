{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T07:35:39.193066Z",
     "iopub.status.busy": "2024-09-16T07:35:39.192780Z",
     "iopub.status.idle": "2024-09-16T07:36:34.343090Z",
     "shell.execute_reply": "2024-09-16T07:36:34.342128Z",
     "shell.execute_reply.started": "2024-09-16T07:35:39.193033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers[torch] datasets\n",
    "!pip install -q bitsandbytes trl peft\n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-16T07:36:34.344931Z",
     "iopub.status.busy": "2024-09-16T07:36:34.344597Z",
     "iopub.status.idle": "2024-09-16T07:36:37.040454Z",
     "shell.execute_reply": "2024-09-16T07:36:37.039539Z",
     "shell.execute_reply.started": "2024-09-16T07:36:34.344896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset as Ds\n",
    "from datasets import *\n",
    "\n",
    "raw_datasets = load_dataset(\"parquet\",data_files={'train': '/kaggle/input/final-data/train.parquet', 'test': '/kaggle/input/final-data/test.parquet'})\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T07:36:37.041887Z",
     "iopub.status.busy": "2024-09-16T07:36:37.041514Z",
     "iopub.status.idle": "2024-09-16T07:36:37.054204Z",
     "shell.execute_reply": "2024-09-16T07:36:37.053355Z",
     "shell.execute_reply.started": "2024-09-16T07:36:37.041855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "example = raw_datasets[\"train\"][0]\n",
    "messages = example[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T07:36:37.058962Z",
     "iopub.status.busy": "2024-09-16T07:36:37.058654Z",
     "iopub.status.idle": "2024-09-16T07:36:37.066437Z",
     "shell.execute_reply": "2024-09-16T07:36:37.065477Z",
     "shell.execute_reply.started": "2024-09-16T07:36:37.058931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][:2]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T07:36:37.067969Z",
     "iopub.status.busy": "2024-09-16T07:36:37.067608Z",
     "iopub.status.idle": "2024-09-16T07:36:37.075210Z",
     "shell.execute_reply": "2024-09-16T07:36:37.074252Z",
     "shell.execute_reply.started": "2024-09-16T07:36:37.067936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "example = raw_datasets[\"train\"][0]\n",
    "messages = example[\"messages\"]\n",
    "for message in messages:\n",
    "  role = message[\"role\"]\n",
    "  content = message[\"content\"]\n",
    "  print('{0:20}:  {1}'.format(role, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T07:36:37.077155Z",
     "iopub.status.busy": "2024-09-16T07:36:37.076358Z",
     "iopub.status.idle": "2024-09-16T07:36:40.929360Z",
     "shell.execute_reply": "2024-09-16T07:36:40.928459Z",
     "shell.execute_reply.started": "2024-09-16T07:36:37.077111Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"ytu-ce-cosmos/Turkish-Llama-8b-DPO-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "  tokenizer.model_max_length = 2048\n",
    "\n",
    "# Set chat template\n",
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T07:36:40.930916Z",
     "iopub.status.busy": "2024-09-16T07:36:40.930584Z",
     "iopub.status.idle": "2024-09-16T07:36:41.353048Z",
     "shell.execute_reply": "2024-09-16T07:36:41.352157Z",
     "shell.execute_reply.started": "2024-09-16T07:36:40.930882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    messages = example[\"messages\"]\n",
    "    # We add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return example\n",
    "\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "raw_datasets = raw_datasets.map(apply_chat_template,\n",
    "                                num_proc=cpu_count(),\n",
    "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
    "                                remove_columns=column_names,\n",
    "                                desc=\"Applying chat template\",)\n",
    "\n",
    "# create the splits\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
    "  print(f\"Sample {index} of the processed training set:\\n\\n{raw_datasets['train'][index]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T07:36:41.354818Z",
     "iopub.status.busy": "2024-09-16T07:36:41.354425Z",
     "iopub.status.idle": "2024-09-16T07:36:41.361424Z",
     "shell.execute_reply": "2024-09-16T07:36:41.360379Z",
     "shell.execute_reply.started": "2024-09-16T07:36:41.354774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T07:36:41.363435Z",
     "iopub.status.busy": "2024-09-16T07:36:41.362983Z",
     "iopub.status.idle": "2024-09-16T07:36:41.375693Z",
     "shell.execute_reply": "2024-09-16T07:36:41.374771Z",
     "shell.execute_reply.started": "2024-09-16T07:36:41.363371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "device_map = \"auto\"#{\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "model_kwargs = dict(\n",
    "#     attn_implementation=False,#\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False, # set to False as we're going to use gradient checkpointing\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T07:36:41.377130Z",
     "iopub.status.busy": "2024-09-16T07:36:41.376794Z",
     "iopub.status.idle": "2024-09-16T07:36:59.645284Z",
     "shell.execute_reply": "2024-09-16T07:36:59.644355Z",
     "shell.execute_reply.started": "2024-09-16T07:36:41.377097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "# path where the Trainer will save its checkpoints and logs\n",
    "trained_model_id = \"TUR4N\"\n",
    "output_dir = 'kaggle/working/' + trained_model_id\n",
    "\n",
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    fp16=True, # specify bf16=True instead when training on GPUs that support bf16 else fp16\n",
    "    bf16=False,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2.0e-05,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=2,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=8, # originally set to 8\n",
    "    per_device_train_batch_size=8, # originally set to 8\n",
    "    #push_to_hub=True,\n",
    "    #hub_model_id=trained_model_id,\n",
    "    # hub_strategy=\"every_save\",\n",
    "    # report_to=\"tensorboard\",\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=None,\n",
    "    seed=42,\n",
    ")\n",
    "# based on config\n",
    "peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model_id,\n",
    "        model_init_kwargs=model_kwargs,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=tokenizer.model_max_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T07:36:59.647089Z",
     "iopub.status.busy": "2024-09-16T07:36:59.646477Z",
     "iopub.status.idle": "2024-09-16T07:37:20.701270Z",
     "shell.execute_reply": "2024-09-16T07:37:20.699410Z",
     "shell.execute_reply.started": "2024-09-16T07:36:59.647051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-16T07:37:20.702152Z",
     "iopub.status.idle": "2024-09-16T07:37:20.702538Z",
     "shell.execute_reply": "2024-09-16T07:37:20.702356Z",
     "shell.execute_reply.started": "2024-09-16T07:37:20.702338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-16T07:37:20.703990Z",
     "iopub.status.idle": "2024-09-16T07:37:20.704340Z",
     "shell.execute_reply": "2024-09-16T07:37:20.704184Z",
     "shell.execute_reply.started": "2024-09-16T07:37:20.704166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-16T07:37:20.706532Z",
     "iopub.status.idle": "2024-09-16T07:37:20.707029Z",
     "shell.execute_reply": "2024-09-16T07:37:20.706795Z",
     "shell.execute_reply.started": "2024-09-16T07:37:20.706770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Sen bir yapay zeka asistanısın. Kullanıcı sana bir görev verecek. Amacın görevi olabildiğince sadık bir şekilde tamamlamak. Görevi yerine getirirken adım adım düşün ve adımlarını gerekçelendir.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"'DEDAŞ U- 2021-23069 638285075731527975.pdf\\\n",
    " yapı ve kredi bankası a.ş.www.yapikredi.com.trticaret sicil numarası: 32736mersis no: 0937002089200741işletmenin merkezi: yapıkredi plazad blok 34330 levent - istanbultel: (0212) 339 70 00faks: (0212) 339 60 001/1 istanbul,\\\n",
    "     12/04/2022yazı no: 22 /opy-mby/1410474250yapı ve kredi bankası a.şbankacılık üssüçayırova 41420 kocaeli - türkiyetel: (0262) 647 10 00faks: (0262) 647 15 47www.yapikredi.com.tr t.c.şanliurfa2. icra dairesiilgi : \\\n",
    "     11/04/2022 tarih ve 2021/23069 nolu yazınız,ilgide kayıtlı yazınızda adı geçen mahmut ketboğa adına tüm şubelerimiz nezdinde 12/04/2022 tarih, 17:07 saat itibariyle herhangi bir hak ve alacağı bulunmadığından hacziniz tatbik edilememiştir.\\\n",
    "     diğer yandan bilindiği üzere haciz kararları çerçevesinde 3. kişinin sorumluluğu haciz bildiriminin kendisine tebliğ edildiği tarihteki mevcut durumu ile sınırlı olup, ileride doğacak veya doğması muhtemel bir hakkın 3. kişi tarafından bilinmesi mümkün\\\n",
    "     olmadığından ; ayrıca haciz kararları borçlunun 3. şahıs nezdindeki mevcut ve bilinen alacakları üzerine uygulanabileceğinden gelecekte doğup doğmayacağı bilinmeyen, ümit ve ihtimale dayalı hak ve alacak üzerinde haciz tatbiki mümkün bulunmamaktadır.\\\n",
    "     açıklanan nedenler ile ilgide kayıtlı haciz kararınıza itiraz ediyoruz. ayrıca, ilgide kayıtlı yazınıza cevap verildiği andan itibaren ileriye yönelik herhangi bir takip yükümlülüğümüz olmadığı hususunu, bilgilerinize arz ederiz.cevaben ve itirazen\\\n",
    "     bildiririz,saygılarımızla,yapı ve kredi bankası a.ş.genel müdürlüğü1410474250 - u005908a'\\\n",
    "     verilen metin içerisinden dosya adı,cevap tarihi,icra dairesi,banka adı ve bakiye listesi bilgilerini çıkart.\"},\n",
    "]\n",
    "\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    ")\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5685098,
     "sourceId": 9373196,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
